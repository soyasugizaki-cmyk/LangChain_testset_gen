import numpy as np
assert np.__version__ == "1.26.4"

import os
import sys
import pickle
import argparse
import re
from pathlib import Path
from typing import List, Dict, Any, Tuple
from datetime import datetime
import pandas as pd
import tiktoken

from dotenv import load_dotenv
load_dotenv(verbose=True)

from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
from langchain_core.documents import Document

# ============================================================
# å®šæ•°
# ============================================================

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
CACHE_DIR = Path("./cache")
TMP_DIR = Path("./tmp")
TESTSET_CACHE_FILE = CACHE_DIR / "testset.pkl"
TESTSET_CSV_FILE = CACHE_DIR / "testset.csv"
CHUNK_MAPPING_FILE = CACHE_DIR / "chunk_mapping.csv"

# ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²è¨­å®š
MIN_CHUNK_TOKENS = 100
EMBEDDING_MODEL = "text-embedding-ada-002"
MAX_EMBEDDING_TOKENS = 8191

# ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®é–¾å€¤ã¨è¨­å®š
CHUNK_SIZE_CONFIG = [
    (500, 2000, 400),    # çŸ­ã„: (é–¾å€¤, chunk_size, overlap)
    (2000, 3000, 600),   # ä¸­ç¨‹åº¦
    (5000, 4000, 800),   # é•·ã„
    (float('inf'), 4000, 800),  # éå¸¸ã«é•·ã„
]

# Azure OpenAIè¨­å®š
AZURE_API_VERSION = "2024-02-15-preview"
AZURE_TIMEOUT = 30
AZURE_MAX_RETRIES = 2

# RAGè¨­å®š
DEFAULT_RETRIEVER_K = 2 # å–å¾—ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯æ•°
TESTSET_SIZE = 10 # ç”Ÿæˆã™ã‚‹ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®æ•°

# ãƒ†ã‚¹ãƒˆç”Ÿæˆæ™‚ã®å¤šæ§˜æ€§ç¢ºä¿è¨­å®š
QUESTION_SIMILARITY_THRESHOLD = 0.85  # è³ªå•ã®é¡ä¼¼åº¦é–¾å€¤ï¼ˆã“ã‚Œä»¥ä¸Šã¯é™¤å¤–ï¼‰
MAX_CHUNK_USAGE_COUNT = 2  # åŒã˜ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰ç”Ÿæˆã§ãã‚‹æœ€å¤§å›æ•°

# Azure OpenAIã®ç’°å¢ƒå¤‰æ•°è¨­å®šã‚’ç¢ºèª
def validate_azure_env_vars():
    """Azure OpenAIã®ç’°å¢ƒå¤‰æ•°ã‚’æ¤œè¨¼"""
    required_vars = {
        "AZURE_OPENAI_API_KEY": "Azure OpenAI APIã‚­ãƒ¼",
        "AZURE_OPENAI_ENDPOINT": "Azure OpenAIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ",
        "AZURE_OPENAI_DEPLOYMENT_NAME": "Azure OpenAIãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆåï¼ˆLLMï¼‰",
        "AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME": "Azure OpenAIãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆåï¼ˆEmbeddingï¼‰",
    }
    
    missing_vars = []
    warnings = []
    
    for var, description in required_vars.items():
        value = os.getenv(var)
        if not value:
            missing_vars.append(f"  - {var}: {description}")
        else:
            # ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒã‚§ãƒƒã‚¯
            if var == "AZURE_OPENAI_ENDPOINT":
                if "/deployments/" in value or "/chat/completions" in value:
                    warnings.append(
                        f"âš ï¸  {var}ã¯å®Œå…¨ãªURLã§ã¯ãªãã€ãƒ™ãƒ¼ã‚¹URLã®ã¿ã‚’æŒ‡å®šã—ã¦ãã ã•ã„\n"
                        f"   èª¤: {value}\n"
                        f"   æ­£: https://your-resource.openai.azure.com/"
                    )
                else:
                    print(f"âœ“ {var}: {value}")
            else:
                print(f"âœ“ {var}: {value[:30]}..." if len(value) > 30 else f"âœ“ {var}: {value}")
    
    if warnings:
        print("\n" + "\n".join(warnings))
    
    if missing_vars:
        print("\nâŒ ä»¥ä¸‹ã®ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“:")
        print("\n".join(missing_vars))
        print("\n.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)
    
    if warnings:
        print("\n.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)
    
    print()

# æ¯å›ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆã—ãªãã¦ã‚‚ã„ã„ã‚ˆã†ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦ãŠã
def get_encoding():
    """ãƒˆãƒ¼ã‚¯ãƒ³ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰"""
    if not hasattr(get_encoding, '_cache'):
        get_encoding._cache = tiktoken.encoding_for_model(EMBEDDING_MODEL)
    return get_encoding._cache

# é©åˆ‡ãªãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®æ±ºå®šã¨ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®åˆ†æ
def get_optimal_chunk_size(documents: List[Document]) -> Tuple[int, int]:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ç‰¹æ€§ã«åŸºã¥ã„ã¦æœ€é©ãªãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’æ±ºå®š"""
    encoding = get_encoding()
    
    # å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—
    doc_tokens = [len(encoding.encode(doc.page_content)) for doc in documents]
    avg_tokens = sum(doc_tokens) / len(doc_tokens) if doc_tokens else 0
    max_tokens = max(doc_tokens) if doc_tokens else 0
    
    # è¨­å®šã‹ã‚‰é©åˆ‡ãªã‚µã‚¤ã‚ºã‚’é¸æŠ
    chunk_size, chunk_overlap = CHUNK_SIZE_CONFIG[-1][1:]  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    for threshold, size, overlap in CHUNK_SIZE_CONFIG:
        if avg_tokens < threshold:
            chunk_size, chunk_overlap = size, overlap
            break
    
    print(f"   ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåˆ†æ:")
    print(f"     - å¹³å‡ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {avg_tokens:.0f}")
    print(f"     - æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {max_tokens}")
    print(f"     - é¸æŠã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {chunk_size}ãƒˆãƒ¼ã‚¯ãƒ³")
    print(f"     - ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—: {chunk_overlap}ãƒˆãƒ¼ã‚¯ãƒ³")
    
    return chunk_size, chunk_overlap

# Azure OpenAIã®LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
def create_azure_llm(**kwargs) -> AzureChatOpenAI:
    """Azure OpenAI LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ"""
    return AzureChatOpenAI(
        azure_deployment=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME"),
        api_version=os.getenv("AZURE_OPENAI_API_VERSION", AZURE_API_VERSION),
        timeout=AZURE_TIMEOUT,
        max_retries=AZURE_MAX_RETRIES,
        **kwargs
    )

# Azure OpenAIã®Embeddingsã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
def create_azure_embeddings(**kwargs) -> AzureOpenAIEmbeddings:
    """Azure OpenAI Embeddingsã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ"""
    return AzureOpenAIEmbeddings(
        azure_deployment=os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME"),
        api_version=os.getenv("AZURE_OPENAI_API_VERSION", AZURE_API_VERSION),
        timeout=AZURE_TIMEOUT,
        max_retries=AZURE_MAX_RETRIES,
        **kwargs
    )

# ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²å™¨ã‚’ä½œæˆ
def create_text_splitter(chunk_size: int, chunk_overlap: int) -> RecursiveCharacterTextSplitter:
    """ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²å™¨ã‚’ä½œæˆ"""
    encoding = get_encoding()
    return RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=lambda text: len(encoding.encode(text)),
        is_separator_regex=False,
        separators=["\n# ", "\n", "ã€‚", "ï¼", ". ", "ï¼", "ï¼Ÿ", "ã€", "ï¼Œ", ", ", " ", ""],
    )

def add_chunk_metadata(chunks: List[Document]) -> Tuple[List[Document], List[Dict]]:
    """ãƒãƒ£ãƒ³ã‚¯ã«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨IDã‚’ä»˜ä¸ï¼ˆpage_contentã«ãƒãƒ¼ã‚«ãƒ¼ã‚‚åŸ‹ã‚è¾¼ã¿ï¼‰"""
    encoding = get_encoding()
    filtered_chunks = []
    chunk_mapping = []
    chunk_counter = 0
    
    for chunk in chunks:
        tokens = len(encoding.encode(chunk.page_content))
        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒå°‘ãªã‘ã‚Œã°é™¤å¤–
        if tokens < MIN_CHUNK_TOKENS:
            continue
        
        source_file = chunk.metadata.get("source", "unknown") # ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«åå–å¾—
        source_filename = Path(source_file).name # ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿å–å¾—
        chunk_id = f"{source_filename}_chunk_{chunk_counter:03d}" # ãƒãƒ£ãƒ³ã‚¯IDç”Ÿæˆ
        
        # page_contentã®å…ˆé ­ã«ãƒãƒ¼ã‚«ãƒ¼ã‚’åŸ‹ã‚è¾¼ã‚€
        chunk.page_content = f"[CHUNK_ID:{chunk_id}]\n{chunk.page_content}"
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
        chunk.metadata.update({
            "chunk_id": chunk_id,
            "chunk_index": chunk_counter,
            "chunk_tokens": tokens,
            "source_file": source_filename,
        })
        
        filtered_chunks.append(chunk)
        chunk_mapping.append({
            "chunk_id": chunk_id,
            "chunk_index": chunk_counter,
            "source_file": source_filename,
            "source_path": source_file,
            "tokens": tokens,
            "content_preview": chunk.page_content[:100].replace("\n", " ")
        })
        chunk_counter += 1
    
    return filtered_chunks, chunk_mapping

def save_chunk_mapping(chunk_mapping: List[Dict]):
    """ãƒãƒ£ãƒ³ã‚¯ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’CSVã«ä¿å­˜"""
    CACHE_DIR.mkdir(exist_ok=True)
    df = pd.DataFrame(chunk_mapping)
    df.to_csv(CHUNK_MAPPING_FILE, index=False, encoding="utf-8")

def save_chunks_to_tmp(filtered_chunks: List[Document]):
    """ãƒãƒ£ãƒ³ã‚¯ã‚’å€‹åˆ¥ã®txtãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦tmpãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜"""
    # tmpãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¦å†ä½œæˆ
    if TMP_DIR.exists():
        import shutil
        shutil.rmtree(TMP_DIR)
    TMP_DIR.mkdir(exist_ok=True)
    
    for chunk in filtered_chunks:
        chunk_id = chunk.metadata.get("chunk_id", "unknown")
        chunk_index = chunk.metadata.get("chunk_index", -1)
        source_file = chunk.metadata.get("source_file", "unknown")
        chunk_tokens = chunk.metadata.get("chunk_tokens", 0)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        filename = TMP_DIR / f"{chunk_id}.txt"
        
        # ãƒãƒ£ãƒ³ã‚¯ã®å†…å®¹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
        with open(filename, "w", encoding="utf-8") as f:
            # ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±
            f.write("=" * 80 + "\n")
            f.write(f"ãƒãƒ£ãƒ³ã‚¯ID: {chunk_id}\n")
            f.write(f"ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«: {source_file}\n")
            f.write(f"ãƒãƒ£ãƒ³ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {chunk_index}\n")
            f.write(f"ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {chunk_tokens}\n")
            f.write("=" * 80 + "\n\n")
            
            # ãƒãƒ£ãƒ³ã‚¯ã®å†…å®¹
            f.write(chunk.page_content)
            f.write("\n")

def print_chunk_stats(original_count: int, filtered_chunks: List[Document], chunk_mapping: List[Dict]):
    """ãƒãƒ£ãƒ³ã‚¯çµ±è¨ˆã‚’è¡¨ç¤º"""
    tokens = [m["tokens"] for m in chunk_mapping]
    removed = original_count - len(filtered_chunks)
    
    print(f"   {original_count}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²")
    if removed > 0:
        print(f"     - {removed}å€‹ã®å°ã•ã„ãƒãƒ£ãƒ³ã‚¯ï¼ˆ<{MIN_CHUNK_TOKENS}ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã‚’é™¤å¤–")
    print(f"     - ä½¿ç”¨ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯: {len(filtered_chunks)}å€‹")
    print(f"     - å¹³å‡ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {sum(tokens)/len(tokens):.0f}ãƒˆãƒ¼ã‚¯ãƒ³")
    if tokens:
        print(f"     - æœ€å°/æœ€å¤§: {min(tokens)}/{max(tokens)}ãƒˆãƒ¼ã‚¯ãƒ³")
    print(f"     - ãƒãƒ£ãƒ³ã‚¯ãƒãƒƒãƒ”ãƒ³ã‚°ä¿å­˜: {CHUNK_MAPPING_FILE}")

# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ã€ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã—ã¦ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä»˜ä¸
def load_documents() -> List[Document]:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ã€ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã—ã¦ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä»˜ä¸"""
    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿
    loader = DirectoryLoader(
        path="./data",
        glob="*.txt",
        loader_cls=TextLoader,
        loader_kwargs={"encoding": "utf-8"},
    )
    documents = loader.load()
    
    # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºæ±ºå®šã¨ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²
    chunk_size, chunk_overlap = get_optimal_chunk_size(documents)
    text_splitter = create_text_splitter(chunk_size, chunk_overlap)
    chunks = text_splitter.split_documents(documents)
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ä¸ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    filtered_chunks, chunk_mapping = add_chunk_metadata(chunks)
    
    # ä¿å­˜ã¨çµ±è¨ˆè¡¨ç¤º
    save_chunk_mapping(chunk_mapping)
    save_chunks_to_tmp(filtered_chunks)
    print(f"   {len(documents)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’", end="")
    print_chunk_stats(len(chunks), filtered_chunks, chunk_mapping)
    print(f"     - ãƒãƒ£ãƒ³ã‚¯å†…å®¹ä¿å­˜: {TMP_DIR}")
    
    return filtered_chunks

# ============================================================
# LangChainç‰ˆãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆç”Ÿæˆ
# ============================================================

# ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ï¼ˆLangChainç‰ˆï¼‰
# ã“ã“ã«ã¯expected_chunk_idsãŒãªã„ãŒã€å¾Œã§è¿½åŠ ã™ã‚‹ã®ã§å•é¡Œãªã„
class TestSet:
    """LangChainç‰ˆã®ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿æ§‹é€ """
    def __init__(self, samples: List[Dict[str, Any]]):
        self.samples = samples
    
    def to_pandas(self):
        """ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚’DataFrameã«å¤‰æ›"""
        data = []
        for sample in self.samples:
            data.append({
                "user_input": sample.get("user_input", ""), # è³ªå•
                "reference_contexts": sample.get("reference_contexts", []), # å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
                "reference": sample.get("reference", ""), # æ­£è§£
                "synthesizer_name": sample.get("synthesizer_name", "langchain"), # ç”Ÿæˆæ–¹æ³•
            })
        return pd.DataFrame(data)

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆ
# ã€å¤‰æ›´ç‚¹ã€‘RAGASã®TestsetGeneratorã‹ã‚‰LangChainã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ+LLMã«å¤‰æ›´
def create_synthesized_test_data(
    documents: List[Document], 
    max_retries: int = 3,
    question_types: List[str] = None
):
    """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆï¼ˆLangChainç‰ˆã€ã‚¨ãƒ©ãƒ¼æ™‚ã¯è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤ï¼‰
    
    RAGASç‰ˆã‹ã‚‰ã®ä¸»ãªå¤‰æ›´:
    - TestsetGeneratorã®ä»£ã‚ã‚Šã«ChatPromptTemplateã¨LLMã‚’ä½¿ç”¨
    - å„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ç›´æ¥è³ªå•ã¨å›ç­”ã‚’ç”Ÿæˆ
    - JSONå½¢å¼ã§å‡ºåŠ›ã‚’å—ã‘å–ã‚Šã€ãƒ‘ãƒ¼ã‚¹ã—ã¦ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«ã‚’ä½œæˆ
    
    Args:
        documents: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
        max_retries: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
        question_types: è³ªå•ã®å‚¾å‘ã®ãƒªã‚¹ãƒˆ
            - "single_hop": å˜ä¸€ãƒ›ãƒƒãƒ—è³ªå•ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
            - "multi_hop": ãƒãƒ«ãƒãƒ›ãƒƒãƒ—è³ªå•ï¼ˆè¤‡æ•°ã®æƒ…å ±ã‚’çµ„ã¿åˆã‚ã›ã‚‹ï¼‰
            - "synonym": åŒç¾©èªã§è¨€ã„æ›ãˆãŸè³ªå•
            - "typo": èª¤å­—ã‚’å«ã‚€è³ªå•
            - "negation": å¦å®šå½¢ã®è³ªå•
    """
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import StrOutputParser
    import json
    import random
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã®è¨­å®š
    if question_types is None:
        question_types = ["single_hop"]
    
    # Azure OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆï¼ˆLangChainç‰ˆï¼‰
    llm = create_azure_llm(
        temperature=0.7,  # å¤šæ§˜ãªè³ªå•ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚å°‘ã—é«˜ã‚ã«è¨­å®š
        model_kwargs={
            "response_format": {"type": "json_object"},  # JSONãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–
        }
    )
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆæ—¥æœ¬èªå¯¾å¿œã€è³ªå•ã®å‚¾å‘ã‚’å‹•çš„ã«çµ„ã¿è¾¼ã‚€ï¼‰
    # LangChainã®ChatPromptTemplateã‚’ä½¿ç”¨ã—ã¦è³ªå•ã¨å›ç­”ã‚’ç”Ÿæˆ
    question_instructions = []
    if "multi_hop" in question_types:
        question_instructions.append("- è¤‡æ•°ã®æƒ…å ±ã‚’çµ„ã¿åˆã‚ã›ã¦æ¨è«–ãŒå¿…è¦ãªè³ªå•ï¼ˆãƒãƒ«ãƒãƒ›ãƒƒãƒ—ï¼‰")
    if "synonym" in question_types:
        question_instructions.append("- åŒç¾©èªã‚„é¡ç¾©èªã‚’ä½¿ã£ã¦è¨€ã„æ›ãˆãŸè³ªå•")
    if "typo" in question_types:
        question_instructions.append("- æ„å›³çš„ãªèª¤å­—ã‚„ã‚¿ã‚¤ãƒ—ãƒŸã‚¹ã‚’å«ã‚€è³ªå•ï¼ˆä¾‹ï¼šã€Œè“‹ç„¶æ€§ã€â†’ã€Œè“®ç„¶æ€§ã€ã€ã€Œå¥‘ç´„ã€â†’ã€Œkç´„ã€ã€ã€Œåˆ©ç”¨ã€â†’ã€Œç†å®¹ã€ãªã©ã€ã‚ˆãã‚ã‚‹èª¤å­—ã‚„å¤‰æ›ãƒŸã‚¹ã‚’å«ã‚ã‚‹ã€‚è³ªå•æ–‡ã«å¿…ãš1ã¤ä»¥ä¸Šã®èª¤å­—ã‚’å«ã‚ã‚‹ã“ã¨ï¼‰")
    if "negation" in question_types:
        question_instructions.append("- å¦å®šå½¢ã‚„åå¯¾ã®æ„å‘³ã‚’å•ã†è³ªå•")
    if "single_hop" in question_types or not question_instructions:
        question_instructions.append("- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ç›´æ¥ç­”ãˆã‚‰ã‚Œã‚‹å˜ç´”ãªè³ªå•ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒ›ãƒƒãƒ—ï¼‰")
    
    question_instructions_text = "\n".join(question_instructions)
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®æ§‹ç¯‰
    # JSONã®ä¾‹ã®ä¸­ã®{question}ã¨{answer}ã¯å¤‰æ•°ã¨ã—ã¦è§£é‡ˆã•ã‚Œãªã„ã‚ˆã†ã«ã€
    # æ–‡å­—åˆ—ãƒªãƒ†ãƒ©ãƒ«ã¨ã—ã¦è¡¨ç¾ï¼ˆ"question"ã¨"answer"ã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼‰
    system_prompt_template = """ã‚ãªãŸã¯RAGã‚·ã‚¹ãƒ†ãƒ ã®è©•ä¾¡ç”¨ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’ç”Ÿæˆã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚
ä¸ãˆã‚‰ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†…å®¹ã‚’åŸºã«ã€ä»¥ä¸‹ã®JSONå½¢å¼ã§è³ªå•ã¨å›ç­”ã®ãƒšã‚¢ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

å‡ºåŠ›å½¢å¼:
{{
    "question": "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†…å®¹ã«é–¢ã™ã‚‹å…·ä½“çš„ãªè³ªå•ï¼ˆæ—¥æœ¬èªï¼‰",
    "answer": "è³ªå•ã«å¯¾ã™ã‚‹æ­£ç¢ºãªå›ç­”ï¼ˆæ—¥æœ¬èªã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†…å®¹ã«åŸºã¥ãï¼‰"
}}

è³ªå•ã¯ä»¥ä¸‹ã®è¦ä»¶ã‚’æº€ãŸã™å¿…è¦ãŒã‚ã‚Šã¾ã™:
- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†…å®¹ã«ç›´æ¥é–¢é€£ã—ã¦ã„ã‚‹
- æ˜ç¢ºã§å…·ä½“çš„ã§ã‚ã‚‹
- å›ç­”ãŒãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰å°ãå‡ºã›ã‚‹
- æ—¥æœ¬èªã§è¨˜è¿°ã•ã‚Œã¦ã„ã‚‹

è³ªå•ã®å‚¾å‘:
{question_instructions_text}"""
    
    # question_instructions_textã‚’å…ˆã«ç½®æ›ï¼ˆformatã§ç½®æ›ï¼‰
    system_prompt_intermediate = system_prompt_template.format(question_instructions_text=question_instructions_text)
    
    
    # ChatPromptTemplateãŒ{question}ã¨{answer}ã‚’å¤‰æ•°ã¨ã—ã¦è§£é‡ˆã™ã‚‹å•é¡Œã‚’å›é¿ã™ã‚‹ãŸã‚ã€
    # ç›´æ¥LLMã‚’å‘¼ã³å‡ºã™é–¢æ•°ã‚’å®šç¾©
    def generate_qa(doc_content: str) -> str:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰è³ªå•ã¨å›ç­”ã‚’ç”Ÿæˆ"""
        from langchain_core.messages import SystemMessage, HumanMessage
        messages = [
            SystemMessage(content=system_prompt_intermediate),
            HumanMessage(content=f"ä»¥ä¸‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰è³ªå•ã¨å›ç­”ã®ãƒšã‚¢ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„:\n\n{doc_content}")
        ]
        # LLMã‚’ç›´æ¥å‘¼ã³å‡ºã—
        response = llm.invoke(messages)
        return response.content if hasattr(response, 'content') else str(response)
    
    # Embeddingsã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆï¼ˆè³ªå•ã®é¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰
    embeddings = create_azure_embeddings()
    
    # è³ªå•ã®é¡ä¼¼åº¦ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹é–¢æ•°
    def is_question_similar(new_question: str, existing_questions: List[str], threshold: float = QUESTION_SIMILARITY_THRESHOLD) -> bool:
        """æ–°ã—ã„è³ªå•ãŒæ—¢å­˜ã®è³ªå•ã¨é¡ä¼¼ã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        if not existing_questions:
            return False
        
        try:
            # æ—¢å­˜ã®è³ªå•ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
            existing_vectors = embeddings.embed_documents(existing_questions)
            # æ–°ã—ã„è³ªå•ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
            new_vector = embeddings.embed_query(new_question)
            
            # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—
            existing_vectors_np = np.array(existing_vectors)
            new_vector_np = np.array(new_vector)
            
            # æ­£è¦åŒ–
            existing_norms = np.linalg.norm(existing_vectors_np, axis=1)
            new_norm = np.linalg.norm(new_vector_np)
            
            # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ = (Aãƒ»B) / (|A| * |B|)
            similarities = np.dot(existing_vectors_np, new_vector_np) / (existing_norms * new_norm)
            
            # æœ€å¤§é¡ä¼¼åº¦ãŒé–¾å€¤ã‚’è¶…ãˆã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            return float(np.max(similarities)) >= threshold
        except Exception as e:
            print(f"   âš ï¸  é¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {str(e)[:100]}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯é¡ä¼¼ã¨ã¿ãªã•ãªã„ï¼ˆå®‰å…¨å´ã«å€’ã™ï¼‰
            return False
    
    # æ®µéšçš„ã«ã‚µã‚¤ã‚ºã‚’æ¸›ã‚‰ã—ã¦ãƒªãƒˆãƒ©ã‚¤
    testset_sizes = [TESTSET_SIZE, max(1, TESTSET_SIZE - 1), 1]
    
    for attempt, size in enumerate(testset_sizes, 1):
        try:
            print(f"   è©¦è¡Œ {attempt}/{len(testset_sizes)}: testset_size={size}")
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠï¼ˆé‡è¤‡ã‚ã‚Šï¼‰
            # LangChainç‰ˆã§ã¯æ˜ç¤ºçš„ã«ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€
            # selected_docs = random.sample(documents, min(size, len(documents))) # sampleã§ã¯é‡è¤‡ãªã—ãªã®ã§ã€chunkæ•°ãŒå°‘ãªã„ã¨ãƒ†ã‚¹ãƒˆã‚‚å°‘ãªããªã‚‹
            selected_docs = random.choices(documents, k=size) # ã“ã†ã™ã‚‹ã“ã¨ã§é‡è¤‡ã‚ã‚Šã§ãƒ†ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ãŒã§ãã‚‹  
            
            test_samples = []
            existing_questions = []  # æ—¢å­˜ã®è³ªå•ã‚’ä¿æŒï¼ˆé¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰
            chunk_usage_count = {}  # ãƒãƒ£ãƒ³ã‚¯ã®ä½¿ç”¨å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            
            for idx, doc in enumerate(selected_docs):
                try:
                    # ãƒãƒ£ãƒ³ã‚¯IDã‚’æŠ½å‡º
                    chunk_id_match = re.search(r'\[CHUNK_ID:([^\]]+)\]', doc.page_content)
                    chunk_id = chunk_id_match.group(1) if chunk_id_match else doc.metadata.get("chunk_id", f"chunk_{idx}")
                    
                    # ãƒãƒ£ãƒ³ã‚¯ã®ä½¿ç”¨é »åº¦ã‚’ãƒã‚§ãƒƒã‚¯
                    chunk_usage_count[chunk_id] = chunk_usage_count.get(chunk_id, 0)
                    if chunk_usage_count[chunk_id] >= MAX_CHUNK_USAGE_COUNT:
                        print(f"   âš ï¸  ã‚µãƒ³ãƒ—ãƒ« {idx+1}: ãƒãƒ£ãƒ³ã‚¯ {chunk_id} ã®ä½¿ç”¨å›æ•°ãŒä¸Šé™ã«é”ã—ã¦ã„ã¾ã™ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                        continue
                    
                    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…å®¹ã‹ã‚‰ãƒãƒ¼ã‚«ãƒ¼ã‚’é™¤å»ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å«ã‚ã‚‹ãŸã‚ï¼‰
                    doc_content = re.sub(r'\[CHUNK_ID:[^\]]+\]\n?', '', doc.page_content)
                    
                    # LLMã§è³ªå•ã¨å›ç­”ã‚’ç”Ÿæˆï¼ˆLangChainç‰ˆï¼‰
                    # ã€å¤‰æ›´ç‚¹ã€‘RAGASç‰ˆã§ã¯TestsetGenerator.generate_with_langchain_docs()ã‚’ä½¿ç”¨ã—ã¦ã„ãŸãŒã€
                    # LangChainç‰ˆã§ã¯å„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«å¯¾ã—ã¦å€‹åˆ¥ã«LLMã‚’å‘¼ã³å‡ºã—
                    # ChatPromptTemplateã®ä»£ã‚ã‚Šã«ã€ç›´æ¥LLMã‚’å‘¼ã³å‡ºã™é–¢æ•°ã‚’ä½¿ç”¨
                    response = generate_qa(doc_content)
                    
                    # JSONã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆLangChainç‰ˆï¼‰
                    # ã€å¤‰æ›´ç‚¹ã€‘RAGASç‰ˆã§ã¯TestsetGeneratorãŒè‡ªå‹•çš„ã«ãƒ‘ãƒ¼ã‚¹ã—ã¦ã„ãŸãŒã€
                    # LangChainç‰ˆã§ã¯æ˜ç¤ºçš„ã«JSONã‚’ãƒ‘ãƒ¼ã‚¹
                    try:
                        qa_data = json.loads(response)
                        question = qa_data.get("question", "")
                        answer = qa_data.get("answer", "")
                        
                        if not question or not answer:
                            print(f"   âš ï¸  ã‚µãƒ³ãƒ—ãƒ« {idx+1}: è³ªå•ã¾ãŸã¯å›ç­”ãŒç©ºã§ã™ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                            continue
                        
                        # è³ªå•ã®é¡ä¼¼åº¦ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆæ—¢å­˜ã®è³ªå•ã¨ä¼¼ã™ãã¦ã„ãªã„ã‹ï¼‰
                        if is_question_similar(question, existing_questions):
                            print(f"   âš ï¸  ã‚µãƒ³ãƒ—ãƒ« {idx+1}: æ—¢å­˜ã®è³ªå•ã¨é¡ä¼¼åº¦ãŒé«˜ã™ãã¾ã™ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                            continue
                        
                        # ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«ã‚’ä½œæˆï¼ˆLangChainç‰ˆã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ï¼‰
                        # ã€å¤‰æ›´ç‚¹ã€‘RAGASç‰ˆã§ã¯TestsetGeneratorãŒè‡ªå‹•çš„ã«Testsetã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆã—ã¦ã„ãŸãŒã€
                        # LangChainç‰ˆã§ã¯è¾æ›¸å½¢å¼ã§æ˜ç¤ºçš„ã«ä½œæˆ
                        test_samples.append({
                            "user_input": question,
                            "reference_contexts": [doc_content],  # å…ƒã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…å®¹
                            "reference": answer,
                            "synthesizer_name": "langchain",
                            "chunk_id": chunk_id,
                            "source_file": doc.metadata.get("source_file", "unknown"),
                        })
                        
                        # æ—¢å­˜ã®è³ªå•ãƒªã‚¹ãƒˆã¨ãƒãƒ£ãƒ³ã‚¯ä½¿ç”¨å›æ•°ã‚’æ›´æ–°
                        existing_questions.append(question)
                        chunk_usage_count[chunk_id] = chunk_usage_count.get(chunk_id, 0) + 1
                        
                        print(f"   âœ“ ã‚µãƒ³ãƒ—ãƒ« {idx+1}/{size} ã‚’ç”Ÿæˆã—ã¾ã—ãŸï¼ˆãƒãƒ£ãƒ³ã‚¯ {chunk_id} ä½¿ç”¨å›æ•°: {chunk_usage_count[chunk_id]}ï¼‰")
                        
                    except json.JSONDecodeError as e:
                        print(f"   âš ï¸  ã‚µãƒ³ãƒ—ãƒ« {idx+1}: JSONãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ - {str(e)[:100]}")
                        continue
                        
                except Exception as e:
                    print(f"   âš ï¸  ã‚µãƒ³ãƒ—ãƒ« {idx+1}: ã‚¨ãƒ©ãƒ¼ - {str(e)[:100]}")
                    continue
            
            if len(test_samples) == 0:
                raise ValueError("ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«ãŒ1ã¤ã‚‚ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸ")
            
            if attempt > 1:
                print(f"   âœ“ ãƒªãƒˆãƒ©ã‚¤æˆåŠŸï¼ˆã‚µã‚¤ã‚º: {len(test_samples)}ï¼‰")
            
            # TestSetã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆï¼ˆLangChainç‰ˆï¼‰
            # ã€å¤‰æ›´ç‚¹ã€‘RAGASç‰ˆã§ã¯TestsetGeneratorãŒTestsetã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã—ã¦ã„ãŸãŒã€
            # LangChainç‰ˆã§ã¯ã‚«ã‚¹ã‚¿ãƒ ã®TestSetã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨
            testset = TestSet(test_samples)
            return testset
            
        except ValueError as e:
            error_msg = str(e)
            print(f"âš ï¸  ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {error_msg}")
            if attempt < len(testset_sizes):
                print(f"   â†’ ã‚µã‚¤ã‚ºã‚’ {testset_sizes[attempt]} ã«æ¸›ã‚‰ã—ã¦å†è©¦è¡Œã—ã¾ã™")
            else:
                print(f"   â†’ ã™ã¹ã¦ã®è©¦è¡ŒãŒå¤±æ•—ã—ã¾ã—ãŸ")
                raise
        
        except Exception as e:
            error_msg = str(e)
            print(f"âš ï¸  äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {error_msg[:150]}...")
            if attempt < len(testset_sizes):
                print(f"   â†’ ã‚µã‚¤ã‚ºã‚’ {testset_sizes[attempt]} ã«æ¸›ã‚‰ã—ã¦å†è©¦è¡Œã—ã¾ã™")
                continue
            else:
                print(f"   â†’ ã™ã¹ã¦ã®è©¦è¡ŒãŒå¤±æ•—ã—ã¾ã—ãŸ")
                raise

def find_chunk_ids_for_contexts(contexts: List[str], documents: List[Document]) -> List[str]:
    """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆã«å¯¾å¿œã™ã‚‹chunk_idã‚’è¦‹ã¤ã‘ã‚‹"""
    chunk_ids = []
    
    for context in contexts:# contextã¯ãƒ†ã‚¹ãƒˆç”Ÿæˆã®éš›ã«ç”¨ã„ãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        # ã¾ãš[CHUNK_ID:xxx]ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ­£è¦è¡¨ç¾ã§æŠ½å‡º
        chunk_id_match = re.search(r'\[CHUNK_ID:([^\]]+)\]', context)
        if chunk_id_match:
            chunk_id = chunk_id_match.group(1)
            chunk_ids.append(chunk_id)
            continue
        
        # ãƒãƒ¼ã‚«ãƒ¼ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å¾“æ¥ã®æ–¹æ³•ã§ãƒãƒƒãƒãƒ³ã‚°
        matched = False
        context_normalized = context.strip()
        
        for doc in documents:# documentsã¯åˆ†å‰²ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
            doc_content = doc.page_content.strip()
            # å®Œå…¨ä¸€è‡´ã¾ãŸã¯é«˜ã„é¡ä¼¼åº¦ã§ãƒãƒƒãƒãƒ³ã‚°
            if context_normalized == doc_content or context_normalized in doc_content:
                chunk_id = doc.metadata.get("chunk_id", "unknown")
                chunk_ids.append(chunk_id)
                matched = True
                break
        
        if not matched:
            chunk_ids.append("unknown")
    
    return chunk_ids

def save_testset_to_cache(testset, documents: List[Document]):
    """ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ï¼ˆæœŸå¾…ã•ã‚Œã‚‹chunk_idä»˜ãã€LangChainç‰ˆï¼‰
    
    ã€å¤‰æ›´ç‚¹ã€‘RAGASç‰ˆã®Testset.samplesã‹ã‚‰ã€LangChainç‰ˆã®TestSet.samplesï¼ˆè¾æ›¸ãƒªã‚¹ãƒˆï¼‰ã«å¯¾å¿œ
    """
    CACHE_DIR.mkdir(exist_ok=True)
    with open(TESTSET_CACHE_FILE, "wb") as f:
        pickle.dump(testset, f)
    print(f"ğŸ’¾ ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ã—ã¾ã—ãŸ: {TESTSET_CACHE_FILE}")

    # DataFrameã«å¤‰æ›
    df_test = testset.to_pandas()
    
    # å„ã‚µãƒ³ãƒ—ãƒ«ã®æœŸå¾…ã•ã‚Œã‚‹chunk_idã‚’æŠ½å‡º
    expected_chunk_ids_list = []
    
    for sample in testset.samples:
        # ã€LangChainç‰ˆå¤‰æ›´ã€‘RAGASç‰ˆã§ã¯sample.eval_sample.reference_context_idsã‚’ä½¿ç”¨ã—ã¦ã„ãŸãŒã€
        # LangChainç‰ˆã§ã¯chunk_idãŒç›´æ¥ã‚µãƒ³ãƒ—ãƒ«è¾æ›¸ã«å«ã¾ã‚Œã¦ã„ã‚‹
        chunk_id = sample.get("chunk_id")
        if chunk_id:
            chunk_ids = [chunk_id]
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: reference_contextsã‹ã‚‰æ¤œç´¢
            contexts = sample.get("reference_contexts", [])
            chunk_ids = find_chunk_ids_for_contexts(contexts, documents)
        
        # ãƒªã‚¹ãƒˆå½¢å¼ã§ä¿å­˜ï¼ˆç©ºã®å ´åˆã¯ç©ºãƒªã‚¹ãƒˆï¼‰
        expected_chunk_ids_list.append(chunk_ids if chunk_ids else [])
    
    # æ–°ã—ã„åˆ—ã‚’è¿½åŠ 
    df_test['expected_chunk_ids'] = expected_chunk_ids_list
    
    # åˆ—ã®é †åºã‚’èª¿æ•´ï¼ˆè¦‹ã‚„ã™ãã™ã‚‹ãŸã‚ï¼‰
    columns_order = ['user_input', 'expected_chunk_ids', 'reference_contexts', 'reference', 'synthesizer_name']
    df_test = df_test[columns_order]
    
    df_test.to_csv(TESTSET_CSV_FILE, index=False)
    print(f"ğŸ’¾ ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚’CSVã«ä¿å­˜ã—ã¾ã—ãŸ: {TESTSET_CSV_FILE}")

def load_testset_from_cache():
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿"""
    if TESTSET_CACHE_FILE.exists():
        with open(TESTSET_CACHE_FILE, "rb") as f:
            testset = pickle.load(f)
        print(f"ğŸ“¦ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {TESTSET_CACHE_FILE}")
        return testset
    return None

def generate_run_id() -> str:
    """å®Ÿè¡Œã”ã¨ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªIDã‚’ç”Ÿæˆ"""
    return datetime.now().strftime("%Y%m%d_%H%M%S")

def create_ls_dataset(run_id: str):
    """å®Ÿè¡ŒIDã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ"""
    from langsmith import Client
    
    # LangSmith APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªï¼ˆæ–°æ—§ä¸¡æ–¹ã‚’ã‚µãƒãƒ¼ãƒˆï¼‰
    api_key = os.getenv("LANGSMITH_API_KEY") or os.getenv("LANGCHAIN_API_KEY")
    if not api_key:
        print("âš ï¸  LANGSMITH_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚LangSmithã¸ã®ä¿å­˜ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return None, None

    # å®Ÿè¡Œã”ã¨ã«ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã‚’ç”Ÿæˆ
    dataset_name = f"agent-book_{run_id}"

    try:
        client = Client()
        dataset = client.create_dataset(
            dataset_name=dataset_name,
            description=f"RAGè©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (å®Ÿè¡ŒID: {run_id})"
        )
        return dataset, dataset_name
    except Exception as e:
        print(f"âš ï¸  LangSmithãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        print("   è©•ä¾¡ã¯ç¶šè¡Œã—ã¾ã™ãŒã€LangSmithã¸ã®ä¿å­˜ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚")
        return None, None

def save_test_data(testset, dataset, run_id: str):
    """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’LangSmithã«ä¿å­˜ï¼ˆå®Ÿè¡ŒIDã¨ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãã€LangChainç‰ˆï¼‰
    
    ã€å¤‰æ›´ç‚¹ã€‘RAGASç‰ˆã®testset_record.eval_sampleã‹ã‚‰ã€LangChainç‰ˆã®è¾æ›¸å½¢å¼ï¼ˆtestset_record.get()ï¼‰ã«å¯¾å¿œ
    """
    if dataset is None:
        print("   LangSmithãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return
    
    from langsmith import Client
    
    try:
        client = Client()
        inputs = []
        outputs = []
        metadatas = []
        
        timestamp = datetime.now().isoformat()

        for idx, testset_record in enumerate(testset.samples):
            # ã€LangChainç‰ˆå¤‰æ›´ã€‘RAGASç‰ˆã§ã¯testset_record.eval_sample.reference_contextsã‚’ä½¿ç”¨ã—ã¦ã„ãŸãŒã€
            # LangChainç‰ˆã§ã¯testset_record.get("reference_contexts")ã§è¾æ›¸ã‹ã‚‰ç›´æ¥å–å¾—
            contexts = testset_record.get("reference_contexts", [])
            
            inputs.append(
                {
                    "question": testset_record.get("user_input", ""),
                }
            )
            outputs.append(
                {
                    "contexts": contexts,
                    "ground_truth": testset_record.get("reference", ""),
                }
            )
            metadatas.append(
                {
                    "run_id": run_id,
                    "timestamp": timestamp,
                    "example_index": idx,
                    "source": "synthesized",
                    "synthesizer": testset_record.get("synthesizer_name", "langchain"),
                }
            )
        
        client.create_examples(
            inputs=inputs,
            outputs=outputs,
            metadata=metadatas,
            dataset_id=dataset.id,
        )
        print(f"âœ“ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ (å®Ÿè¡ŒID: {run_id})\n")
    except Exception as e:
        print(f"âš ï¸  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        print("   è©•ä¾¡ã¯ç¶šè¡Œã—ã¾ã™ã€‚")


def get_evaluator():
    """RAGè©•ä¾¡å™¨ã‚’ä½œæˆ"""
    from ragas.metrics import answer_relevancy, context_precision, context_recall, faithfulness
    from evaluator import RagasMetricEvaluator

    llm = create_azure_llm(temperature=0)
    embeddings = create_azure_embeddings()
    metrics = [context_precision, answer_relevancy, context_recall, faithfulness] # ã€€é‡‘ã‹ã‹ã‚‹ã‹ã‚‰ä¸€æ—¦æŠœãã§

    return [RagasMetricEvaluator(m, llm, embeddings).evaluate for m in metrics]


def create_rag_chain(documents: List[Document]):
    """RAGãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆ"""
    from langchain_chroma import Chroma
    from langchain_core.output_parsers import StrOutputParser
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.runnables import RunnableParallel, RunnablePassthrough
    
    # ChromaDBã®ãƒ†ãƒ¬ãƒ¡ãƒˆãƒªãƒ¼ã‚’ç„¡åŠ¹åŒ–
    os.environ["ANONYMIZED_TELEMETRY"] = "False"
    
    # ãƒ™ã‚¯ãƒˆãƒ«DBæ§‹ç¯‰
    embeddings = create_azure_embeddings()
    chunk_ids = [doc.metadata.get("chunk_id", f"chunk_{i}") for i, doc in enumerate(documents)]
    db = Chroma.from_documents(documents, embeddings, ids=chunk_ids)
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¢ãƒ‡ãƒ«
    prompt = ChatPromptTemplate.from_template(
        "ä»¥ä¸‹ã®æ–‡è„ˆã ã‘ã‚’è¸ã¾ãˆã¦è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚\n\n"
        "æ–‡è„ˆ: \"\"\"\n{context}\n\"\"\"\n\nè³ªå•: {question}"
    )
    model = create_azure_llm(temperature=0)
    
    # Retrieverè¨­å®š
    k = min(DEFAULT_RETRIEVER_K, len(documents))
    retriever = db.as_retriever(search_kwargs={"k": k})
    
    # ãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰
    return RunnableParallel({
        "question": RunnablePassthrough(),
        "context": retriever,
    }).assign(answer=prompt | model | StrOutputParser())

def extract_context_metadata(contexts: List[Document]) -> List[Dict]:
    """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
    return [{
        "chunk_id": doc.metadata.get("chunk_id", "unknown"),
        "source_file": doc.metadata.get("source_file", "unknown"),
        "chunk_index": doc.metadata.get("chunk_index", -1),
        "content": doc.page_content
    } for doc in contexts]

def infer(evaluators, documents: List[Document], dataset_name: str, run_id: str):
    """æ¨è«–ã¨è©•ä¾¡ã‚’å®Ÿè¡Œ"""
    # LangSmith APIã‚­ãƒ¼ç¢ºèª
    api_key = os.getenv("LANGSMITH_API_KEY") or os.getenv("LANGCHAIN_API_KEY")
    if not api_key:
        print("âš ï¸  LANGSMITH_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€è©•ä¾¡ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return None
    
    # RAGãƒã‚§ãƒ¼ãƒ³ä½œæˆ
    chain = create_rag_chain(documents)
    
    def predict(inputs: Dict[str, Any]) -> Dict[str, Any]:
        output = chain.invoke(inputs["question"])
        context_metadata = extract_context_metadata(output["context"])
        
        return {
            "contexts": output["context"],
            "answer": output["answer"],
            "retrieved_chunk_ids": [m["chunk_id"] for m in context_metadata],
            "context_metadata": context_metadata,
            "run_id": run_id,  # å®Ÿè¡ŒIDã‚’å«ã‚ã‚‹
        }
    
    try:
        from langsmith.evaluation import evaluate
        return evaluate(
            predict, 
            data=dataset_name, 
            evaluators=evaluators,
            experiment_prefix=f"rag-eval-{run_id}"  # å®Ÿé¨“åã«ã‚‚run_idã‚’å«ã‚ã‚‹
        )
    except Exception as e:
        print(f"âš ï¸  è©•ä¾¡ã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None


def main():
    parser = argparse.ArgumentParser(description="RAGè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ")
    parser.add_argument(
        "--skip-generation",
        action="store_true",
        help="ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã‚€",
    )
    parser.add_argument(
        "--regenerate",
        action="store_true",
        help="ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡è¦–ã—ã¦ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å†ç”Ÿæˆ",
    )
    parser.add_argument(
        "--only-generate",
        action="store_true",
        help="ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆã®ã¿ã‚’å®Ÿè¡Œï¼ˆè©•ä¾¡ã¯å®Ÿè¡Œã—ãªã„ï¼‰",
    )
    parser.add_argument(
        "--skip-evaluation",
        action="store_true",
        help="è©•ä¾¡ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆãƒ»ä¿å­˜ã®ã¿å®Ÿè¡Œï¼‰",
    )
    parser.add_argument(
        "--question-types",
        nargs="+",
        default=["single_hop"],
        choices=["single_hop", "multi_hop", "synonym", "typo", "negation"],
        help="è³ªå•ã®å‚¾å‘ã‚’æŒ‡å®šï¼ˆè¤‡æ•°æŒ‡å®šå¯ï¼‰: single_hop, multi_hop, synonym, typo, negation",
    )
    args = parser.parse_args()
    validate_azure_env_vars()
    
    # å®Ÿè¡Œã”ã¨ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªIDã‚’ç”Ÿæˆ
    run_id = generate_run_id()
    print(f"ğŸ†” å®Ÿè¡ŒID: {run_id}\n")
    
    print("ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
    documents = load_documents()
    print(f"âœ“ {len(documents)}å€‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ\n")
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆã¾ãŸã¯èª­ã¿è¾¼ã¿
    testset = None
    if args.skip_generation and not args.regenerate:
        testset = load_testset_from_cache()
        if testset:
            print(f"âœ“ {len(testset.samples)}å€‹ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ\n")
        else:
            print("âš ï¸  ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã™...\n")
    
    if testset is None or args.regenerate:
        print("ğŸ”¬ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™...")
        if args.question_types:
            print(f"   è³ªå•ã®å‚¾å‘: {', '.join(args.question_types)}")
        try:
            testset = create_synthesized_test_data(documents, question_types=args.question_types)
            print(f"âœ“ {len(testset.samples)}å€‹ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’ç”Ÿæˆã—ã¾ã—ãŸ\n")
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ï¼ˆchunk_idæƒ…å ±ã‚‚å«ã‚ã‚‹ï¼‰
            save_testset_to_cache(testset, documents)
            print()
        except Exception as e:
            print(f"\nâŒ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            
            # æ—¢å­˜ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç¢ºèª
            cached_testset = load_testset_from_cache()
            if cached_testset:
                print("   â†’ æ—¢å­˜ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ã—ã¾ã™")
                testset = cached_testset
            else:
                print("   â†’ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚‚è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚è©•ä¾¡ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                print("\n" + "=" * 50)
                print("âš ï¸  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™")
                print("=" * 50)
                return
    
    # LangSmithã¸ã®ä¿å­˜
    dataset = None
    dataset_name = None
    if not args.only_generate:
        print("ğŸ“Š LangSmithãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ã¦ã„ã¾ã™...")
        dataset, dataset_name = create_ls_dataset(run_id)
        if dataset:
            print(f"âœ“ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ '{dataset.name}' ã‚’ä½œæˆã—ã¾ã—ãŸ\n")
        else:
            print("   LangSmithãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ\n")
        
        print("ğŸ’¾ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’LangSmithã«ä¿å­˜ã—ã¦ã„ã¾ã™...")
        save_test_data(testset, dataset, run_id)
    
    if args.only_generate:
        print("=" * 50)
        print("âœ“ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸ")
        print("=" * 50)
        return
    
    # è©•ä¾¡ã®å®Ÿè¡Œ
    if not args.skip_evaluation:
        if dataset_name is None:
            print("âš ï¸  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒä½œæˆã•ã‚Œã¦ã„ãªã„ãŸã‚ã€è©•ä¾¡ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™\n")
        else:
            print("âš™ï¸  è©•ä¾¡å™¨ã‚’åˆæœŸåŒ–ã—ã¦ã„ã¾ã™...")
            evaluators = get_evaluator()
            print("âœ“ è©•ä¾¡å™¨ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ\n")
            
            print("ğŸš€ æ¨è«–ã¨è©•ä¾¡ã‚’å®Ÿè¡Œã—ã¦ã„ã¾ã™...")
            result = infer(evaluators, documents, dataset_name, run_id)
            
            if result:
                print("âœ“ è©•ä¾¡ãŒå®Œäº†ã—ã¾ã—ãŸ\n")
                print("=" * 50)
                print("ğŸ“ˆ è©•ä¾¡çµæœ:")
                print("=" * 50)
                
                # çµæœã‚’DataFrameã«å¤‰æ›ã—ã¦è¡¨ç¤º
                import pandas as pd
                df = result.to_pandas()
                # print(df.to_string())
                
                # CSVä¿å­˜
                #result_csv = CACHE_DIR / f"evaluation_results_{run_id}.csv"
                #df.to_csv(result_csv, index=False)
                #print(f"\nğŸ’¾ è©•ä¾¡çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {result_csv}")
                
                # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
                if 'feedback.context_precision' in df.columns and 'feedback.answer_relevancy' in df.columns:
                    print("\nğŸ“Š ã‚¹ã‚³ã‚¢ã‚µãƒãƒªãƒ¼:")
                    print(f"  - Context Precision å¹³å‡: {df['feedback.context_precision'].mean():.3f}")
                    print(f"  - Answer Relevancy å¹³å‡: {df['feedback.answer_relevancy'].mean():.3f}")
            else:
                print("âš ï¸  è©•ä¾¡ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸï¼ˆLangSmith APIã‚­ãƒ¼ãŒæœªè¨­å®šï¼‰\n")
    else:
        print("=" * 50)
        print("âœ“ è©•ä¾¡ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")
        print("=" * 50)

if __name__ == "__main__":
    main()